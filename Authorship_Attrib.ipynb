{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.wholeTextFiles(\"C:/Users/RAMA/Downloads/MSBA/Dr. Nerur- Big Data/Data3/*.txt\",use_unicode=True)\n",
    "#data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(text):\n",
    "\tregex = re.compile(r'[\\ufeff\\n\\r\\t]')\n",
    "\tt = regex.sub(\" \", text[1])\n",
    "\tq = re.sub(r'\\d', ' ', t)\n",
    "\treturn (text[0],q)\n",
    "\n",
    "def parse_text1(text):\n",
    "\ttext = text.replace('\\ufeff', ' ')\n",
    "\ttext = re.sub(r'[-!$%^*&>:;,.?#/@\"()\\[\\]]',' ',text.lower().replace('\\r\\n', ' '))\n",
    "\ttext = re.sub(r'\\d', ' ', text)\n",
    "\ttext = re.sub(r'twain|chapter',' ',text)\n",
    "\ttext = ' '.join(text.split())\n",
    "\treturn text\n",
    "\n",
    "def filter_test(text, lst):\n",
    "\tfile = text[0]\n",
    "\tbg = text[1]\n",
    "\tres=[]\n",
    "\tfor i in lst:\n",
    "\t\tfor j in bg:\n",
    "\t\t\tif i==j:\n",
    "\t\t\t\tres.append(i)\n",
    "\treturn (file,res)\n",
    "\n",
    "def token_to_pos(ch):\n",
    "\ttokens = nltk.word_tokenize(ch[1])\n",
    "\treturn [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "def Syn(ch):\n",
    "\tchapters_pos = token_to_pos(ch)\n",
    "\tpos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "\tfvs_syntax = [[ch.count(pos) for pos in pos_list] for ch in chapters_pos]\n",
    "\tfvs_syntax = fvs_syntax[0]\n",
    "\t\n",
    "\tresult=[]\n",
    "\t\n",
    "\tfor i in fvs_syntax:\n",
    "\t\tfor j in chapters_pos:\n",
    "\t\t\tresult.append(float(format(float(i/len(j)),'.4f')))\n",
    "\t\n",
    "\tr1=result[0]\n",
    "\tr2=result[1]\n",
    "\tr3=result[2]\n",
    "\tr4=result[3]\n",
    "\tr5=result[4]\n",
    "\tr6=result[5]\n",
    "\t\n",
    "\treturn (ch[0],r1,r2,r3,r4,r5,r6)\n",
    "\n",
    "def LexicalFeatures(chapters):\n",
    "\tsentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\tword_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\t    \n",
    "\ttokens = nltk.word_tokenize(chapters[1].lower())\n",
    "\twords = word_tokenizer.tokenize(chapters[1].lower())\n",
    "\tsentences = sentence_tokenizer.tokenize(chapters[1])\n",
    "\tvocab = set(words)\n",
    "\t\n",
    "\twords_per_sentence = np.array([len(word_tokenizer.tokenize(s)) for s in sentences])\n",
    "\tfvs_lexical_0 = format(words_per_sentence.mean(),'.4f')\n",
    "\tfvs_lexical_1 = format(words_per_sentence.std(),'.4f')\n",
    "\tfvs_lexical_2 = format(len(vocab) / float(len(words)),'.4f')\n",
    "\tfvs_punct_0 = format(tokens.count(',') / float(len(sentences)),'.4f')\n",
    "\tfvs_punct_1 = format(tokens.count(';') / float(len(sentences)),'.4f')\n",
    "\tfvs_punct_2 = format(tokens.count(':') / float(len(sentences)),'.4f')\n",
    "\t\n",
    "\treturn (chapters[0],float(fvs_lexical_0), float(fvs_lexical_1),float(fvs_lexical_2),float(fvs_punct_0),float(fvs_punct_1),float(fvs_punct_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_parse = data.map(lambda x: (x[0].split(\"/\")[-1],parse_text1(x[1])))\n",
    "#data_parse.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textDF = spark.createDataFrame(data_parse, [\"label\", \"text\"])\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "wordsDataFrame = tokenizer.transform(textDF)\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"stwords\")\n",
    "DF = remover.transform(wordsDataFrame)\n",
    "\n",
    "pair = DF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "stp = pair.map(lambda x: x[0]).collect()[0:20]\n",
    "#remover = StopWordsRemover(inputCol=\"stwords\", outputCol=\"finalword\", stopWords=stp)\n",
    "#FDF = remover.transform(DF)\n",
    "filrdd = DF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_test(l,stp))\n",
    "FDF = spark.createDataFrame(filrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "model = cv.fit(FDF)\n",
    "result = model.transform(FDF)\n",
    "finaldata = result.select(\"label\",\"features\")\n",
    "\n",
    "word_df = finaldata.rdd.map(lambda x: Row(Doc_Name=x[0],f1=float(x[1].toArray()[0]),f2=float(x[1].toArray()[1]),f3=float(x[1].toArray()[2]),f4=float(x[1].toArray()[3]),f5=float(x[1].toArray()[4]),f6=float(x[1].toArray()[5]),f7=float(x[1].toArray()[6]),f8=float(x[1].toArray()[7]),f9=float(x[1].toArray()[8]),f10=float(x[1].toArray()[9]),f11=float(x[1].toArray()[10]),f12=float(x[1].toArray()[11]),f13=float(x[1].toArray()[12]),f14=float(x[1].toArray()[13]),f15=float(x[1].toArray()[14]),f16=float(x[1].toArray()[15]),f17=float(x[1].toArray()[16]),f18=float(x[1].toArray()[17]),f19=float(x[1].toArray()[18]),f20=float(x[1].toArray()[19]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "bigramDF = bigram.transform(wordsDataFrame)\n",
    "\n",
    "bipair = bigramDF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "bistp = bipair.map(lambda x: x[0]).collect()[0:10]\n",
    "# biremover = StopWordsRemover(inputCol=\"bigrams\", outputCol=\"finalword\", stopWords=bistp)\n",
    "# biFDF = biremover.transform(bigramDF)\n",
    "bifilrdd = bigramDF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_test(l,bistp))\n",
    "biFDF = spark.createDataFrame(bifilrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "\n",
    "bicv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "bimodel = cv.fit(biFDF)\n",
    "biresult = bimodel.transform(biFDF)\n",
    "bifinaldata = biresult.select(\"label\",\"features\")\n",
    "\n",
    "bigram_df = bifinaldata.rdd.map(lambda x: Row(Doc_Name=x[0],bg1=float(x[1].toArray()[0]),bg2=float(x[1].toArray()[1]),bg3=float(x[1].toArray()[2]),bg4=float(x[1].toArray()[3]),bg5=float(x[1].toArray()[4]),bg6=float(x[1].toArray()[5]),bg7=float(x[1].toArray()[6]),bg8=float(x[1].toArray()[7]),bg9=float(x[1].toArray()[8]),bg10=float(x[1].toArray()[9]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram = NGram(n=3, inputCol=\"words\", outputCol=\"trigrams\")\n",
    "trigramDF = trigram.transform(wordsDataFrame)\n",
    "\n",
    "tripair = trigramDF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "tristp = tripair.map(lambda x: x[0]).collect()[0:10]\n",
    "# triremover = StopWordsRemover(inputCol=\"trigrams\", outputCol=\"finalword\", stopWords=tristp)\n",
    "# triFDF = triremover.transform(trigramDF)\n",
    "trifilrdd = trigramDF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_test(l,tristp))\n",
    "triFDF = spark.createDataFrame(trifilrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "tricv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "trimodel = cv.fit(triFDF)\n",
    "triresult = trimodel.transform(triFDF)\n",
    "trifinaldata = triresult.select(\"label\",\"features\")\n",
    "\n",
    "trigram_df = trifinaldata.rdd.map(lambda x: Row(Doc_Name=x[0],tg1=float(x[1].toArray()[0]),tg2=float(x[1].toArray()[1]),tg3=float(x[1].toArray()[2]),tg4=float(x[1].toArray()[3]),tg5=float(x[1].toArray()[4]),tg6=float(x[1].toArray()[5]),tg7=float(x[1].toArray()[6]),tg8=float(x[1].toArray()[7]),tg9=float(x[1].toArray()[8]),tg10=float(x[1].toArray()[9]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qgram = NGram(n=4, inputCol=\"words\", outputCol=\"qgrams\")\n",
    "qgramDF = qgram.transform(wordsDataFrame)\n",
    "\n",
    "qpair = qgramDF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "qstp = qpair.map(lambda x: x[0]).collect()[0:10]\n",
    "# qremover = StopWordsRemover(inputCol=\"qgrams\", outputCol=\"finalword\", stopWords=qstp)\n",
    "# qFDF = qremover.transform(qgramDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qfilrdd = qgramDF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_test(l,qstp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qFDF = spark.createDataFrame(qfilrdd, [\"label\", \"finalword\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qcv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "qmodel = cv.fit(qFDF)\n",
    "qresult = qmodel.transform(qFDF)\n",
    "qfinaldata = qresult.select(\"label\",\"features\")\n",
    "\n",
    "qgram_df = qfinaldata.rdd.map(lambda x: Row(Doc_Name=x[0],qg1=float(x[1].toArray()[0]),qg2=float(x[1].toArray()[1]),qg3=float(x[1].toArray()[2]),qg4=float(x[1].toArray()[3]),qg5=float(x[1].toArray()[4]),qg6=float(x[1].toArray()[5]),qg7=float(x[1].toArray()[6]),qg8=float(x[1].toArray()[7]),qg9=float(x[1].toArray()[8]),qg10=float(x[1].toArray()[9]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = data.map(lambda x: (x[0].split(\"/\")[-1],x[1].lower()))\n",
    "\n",
    "data_new = data2.map(parse_text)\n",
    "\n",
    "lexical=data_new.map(LexicalFeatures)\n",
    "\n",
    "syntactic=data_new.map(Syn)\n",
    "\n",
    "lexical_df=spark.createDataFrame(lexical, ['Doc_Name','wrds_sent_mean', 'wrds_sent_std','Lex_diver','Commas_sent','Semicolon_sent','Colons_sent'])\n",
    "syntactic_df=spark.createDataFrame(syntactic, ['Doc_Name','NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexical_df.createOrReplaceTempView(\"lexical_df\")\n",
    "syntactic_df.createOrReplaceTempView(\"syntactic_df\")\n",
    "word_df.createOrReplaceTempView(\"word_df\")\n",
    "bigram_df.createOrReplaceTempView(\"bigram_df\")\n",
    "trigram_df.createOrReplaceTempView(\"trigram_df\")\n",
    "qgram_df.createOrReplaceTempView(\"qgram_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df=spark.sql(\" SELECT CASE WHEN ld.Doc_Name == 'Austen1.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen2.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen3.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen4.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen5.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens1.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens2.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens3.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens4.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens5.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain1.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain2.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain3.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain4.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain5.txt' THEN '2' END Author,\\\n",
    "                                 ld.wrds_sent_mean, ld.wrds_sent_std, ld.Lex_diver, ld.Commas_sent, ld.Semicolon_sent, ld.Colons_sent, \\\n",
    "                                 sd.NN, sd.NNP, sd.DT, sd.IN, sd.JJ, sd.NNS, wd.f1, wd.f2,  wd.f3, wd.f4, wd.f5, wd.f6, wd.f7, wd.f8, \\\n",
    "                                 wd.f9, wd.f10, wd.f11, wd.f12, wd.f13, wd.f14, wd.f15, wd.f16, wd.f17, wd.f18, wd.f19, wd.f20, bd.bg1, \\\n",
    "                                 bd.bg2, bd.bg3, bd.bg4, bd.bg5, bd.bg6, bd.bg7, bd.bg8, bd.bg9, bd.bg10, td.tg1, td.tg2, td.tg3, td.tg4, \\\n",
    "                                 td.tg5, td.tg6, td.tg7, td.tg8, td.tg9, td.tg10, qd.qg1, qd.qg2, qd.qg3, qd.qg4, qd.qg5, qd.qg6, qd.qg7, \\\n",
    "                                 qd.qg8, qd.qg9, qd.qg10,ld.Doc_Name \\\n",
    "                      FROM lexical_df ld, syntactic_df sd, word_df wd, bigram_df bd, trigram_df td, qgram_df qd \\\n",
    "                      WHERE ld.Doc_Name = sd.Doc_Name and ld.Doc_Name = wd.Doc_Name \\\n",
    "                        and ld.Doc_Name = bd.Doc_Name and ld.Doc_Name = td.Doc_Name and ld.Doc_Name = qd.Doc_Name \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df.createOrReplaceTempView(\"final_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_df=spark.sql(\"SELECT Author, wrds_sent_mean, wrds_sent_std, Lex_diver, Commas_sent, Semicolon_sent, Colons_sent, NN, NNP, DT, IN, JJ, NNS, \\\n",
    "                           f1, f2,  f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, bg1, bg2, bg3, bg4, bg5, \\\n",
    "                           bg6, bg7, bg8, bg9, bg10, tg1, tg2, tg3, tg4, tg5, tg6, tg7, tg8, tg9, tg10, qg1, qg2, qg3, qg4, qg5, qg6, qg7, qg8, \\\n",
    "                           qg9, qg10 FROM final_df WHERE Doc_Name not in ('Austen2.txt', 'Dickens3.txt', 'Twain5.txt')\")\n",
    "\n",
    "test_df=spark.sql(\"SELECT Author, wrds_sent_mean, wrds_sent_std, Lex_diver, Commas_sent, Semicolon_sent, Colons_sent, NN, NNP, DT, IN, JJ, NNS, \\\n",
    "                           f1, f2,  f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, bg1, bg2, bg3, bg4, bg5, \\\n",
    "                           bg6, bg7, bg8, bg9, bg10, tg1, tg2, tg3, tg4, tg5, tg6, tg7, tg8, tg9, tg10, qg1, qg2, qg3, qg4, qg5, qg6, qg7, qg8, \\\n",
    "                           qg9, qg10 FROM final_df WHERE Doc_Name in ('Austen2.txt', 'Dickens3.txt', 'Twain5.txt')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.repartition(1).write.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').save(\"C:/Bigdata/BookTraining.csv\")\n",
    "test_df.repartition(1).write.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').save(\"C:/Bigdata/BookTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingdf = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('C:/Bigdata/Book_Training.csv')\n",
    "testdf = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('C:/Bigdata/Book_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training= trainingdf.rdd.map(lambda row: LabeledPoint(row[0], row[1:]))\n",
    "test= testdf.rdd.map(lambda row: LabeledPoint(row[0], row[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(2.0, [34.2479,26.7626,0.3205,2.3223,0.595,0.1405,0.5,0.5,0.5,0.5,0.5,0.5,20.0,3.0,0.0,0.0,25.0,10.0,5.0,0.0,3.0,0.0,4.0,11.0,0.0,5.0,2.0,2.0,7.0,10.0,1.0,0.0,66.0,26.0,13.0,18.0,7.0,0.0,4.0,19.0,5.0,3.0,0.0,0.0,0.0,3.0,2.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [23.9329,16.993,0.2398,1.9597,0.3557,0.0805,0.5,0.5,0.5,0.5,0.5,0.5,0.0,11.0,6.0,0.0,11.0,25.0,24.0,0.0,8.0,0.0,2.0,4.0,19.0,4.0,1.0,4.0,5.0,7.0,3.0,3.0,12.0,7.0,3.0,7.0,14.0,0.0,7.0,7.0,3.0,15.0,0.0,0.0,1.0,0.0,0.0,0.0,6.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [32.5352,23.0326,0.2671,2.8451,0.5282,0.0704,0.5,0.5,0.5,0.5,0.5,1.0,0.0,10.0,2.0,0.0,15.0,20.0,20.0,0.0,9.0,37.0,0.0,7.0,2.0,3.0,5.0,11.0,3.0,10.0,0.0,13.0,22.0,23.0,10.0,11.0,19.0,0.0,7.0,7.0,4.0,9.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(2.0, [24.8122,17.1386,0.3193,1.6133,0.3315,0.0166,0.0,0.0,0.0,0.0,0.0,0.0,4.0,2.0,8.0,7.0,28.0,5.0,15.0,0.0,8.0,1.0,7.0,7.0,0.0,3.0,3.0,8.0,2.0,8.0,2.0,7.0,42.0,23.0,10.0,10.0,8.0,0.0,7.0,10.0,10.0,3.0,0.0,0.0,2.0,2.0,4.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [25.2607,22.9148,0.2293,2.344,0.2564,0.0171,0.0,0.0,0.0,0.0,0.0,0.0,327.0,246.0,119.0,15.0,21.0,21.0,10.0,0.0,24.0,75.0,18.0,34.0,1.0,17.0,16.0,11.0,16.0,29.0,15.0,3.0,103.0,43.0,43.0,49.0,14.0,105.0,10.0,20.0,31.0,32.0,54.0,42.0,2.0,2.0,3.0,0.0,0.0,19.0,2.0,0.0,18.0,17.0,2.0,0.0,14.0,12.0,0.0,0.0,8.0,8.0]),\n",
       " LabeledPoint(2.0, [21.694,15.0759,0.2106,1.3497,0.2459,0.0383,0.3333,0.5,0.5,0.3333,0.5,0.5,0.0,1.0,32.0,34.0,4.0,18.0,12.0,0.0,5.0,1.0,0.0,0.0,0.0,8.0,4.0,10.0,4.0,0.0,11.0,10.0,12.0,21.0,8.0,13.0,4.0,0.0,15.0,8.0,5.0,2.0,0.0,0.0,4.0,1.0,0.0,0.0,2.0,0.0,0.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(2.0, [27.494,24.0968,0.3139,2.4345,0.2619,0.0893,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7.0,93.0,18.0,13.0,3.0,0.0,8.0,2.0,3.0,12.0,1.0,16.0,7.0,7.0,15.0,6.0,7.0,3.0,25.0,26.0,17.0,8.0,10.0,0.0,5.0,5.0,2.0,3.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [25.4818,20.5277,0.2959,1.8773,0.1727,0.0045,0.5,0.5,0.5,0.5,0.5,0.5,0.0,32.0,2.0,0.0,15.0,10.0,3.0,0.0,18.0,16.0,15.0,9.0,1.0,6.0,8.0,8.0,15.0,7.0,6.0,13.0,47.0,36.0,18.0,13.0,17.0,0.0,1.0,14.0,18.0,12.0,0.0,0.0,0.0,3.0,2.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [15.0116,14.4273,0.2022,1.3666,0.1995,0.0464,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8.0,95.0,0.0,51.0,42.0,33.0,182.0,24.0,6.0,52.0,23.0,2.0,34.0,30.0,26.0,33.0,16.0,21.0,15.0,58.0,75.0,29.0,26.0,26.0,0.0,48.0,19.0,23.0,20.0,0.0,0.0,8.0,5.0,1.0,0.0,3.0,0.0,3.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [28.9605,20.4421,0.2226,2.1186,0.4124,0.0452,0.5,0.5,0.3333,0.5,0.5,0.3333,4.0,53.0,13.0,45.0,16.0,20.0,22.0,0.0,13.0,4.0,6.0,14.0,3.0,9.0,9.0,12.0,2.0,14.0,10.0,21.0,21.0,12.0,11.0,7.0,13.0,0.0,22.0,6.0,4.0,12.0,0.0,0.0,5.0,0.0,3.0,0.0,3.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [25.9219,23.1971,0.3068,2.0,0.3906,0.0156,0.5,0.5,0.5,0.5,0.3333,0.3333,0.0,22.0,10.0,64.0,7.0,4.0,0.0,0.0,5.0,1.0,1.0,5.0,2.0,1.0,3.0,5.0,1.0,1.0,10.0,1.0,5.0,3.0,2.0,1.0,5.0,0.0,3.0,0.0,3.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [19.3016,14.6446,0.3302,2.2143,0.0794,0.0238,0.0,0.0,0.0,0.0,0.0,0.0,58.0,11.0,19.0,0.0,9.0,14.0,1.0,0.0,7.0,19.0,14.0,4.0,0.0,4.0,6.0,1.0,0.0,2.0,3.0,2.0,8.0,8.0,4.0,8.0,7.0,18.0,2.0,7.0,1.0,3.0,5.0,10.0,0.0,1.0,2.0,0.0,0.0,0.0,10.0,0.0,0.0,0.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = LogisticRegressionWithLBFGS.train(training, numClasses=3)\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "modelNB = NaiveBayes.train(training)\n",
    "predictionAndLabelsNB = training.map(lambda l: (float(modelNB.predict(l.features)), l.label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\mllib\\evaluation.py:237: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Naive Bayes : \n",
      "\n",
      "[[ 4.  0.  0.]\n",
      " [ 1.  3.  0.]\n",
      " [ 0.  0.  4.]]\n",
      "\n",
      "Summary Of Statistics For Naive Bayes\n",
      "\n",
      "Total Accuracy  : 0.92\n",
      "\n",
      "Total Precision : 0.92\t Precision 1: 0.80\t Precision 2: 1.00\t Precision 3: 1.00\n",
      "\n",
      "Total Recall    : 0.92\t Recall 1   : 1.00\t Recall 2   : 0.75\t Recall 3   : 1.00\n",
      "\n",
      "Total fMeasure  : 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\mllib\\evaluation.py:249: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n",
      "c:\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\mllib\\evaluation.py:262: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"
     ]
    }
   ],
   "source": [
    "modelNB = NaiveBayes.train(training)\n",
    "predictionAndLabelsNB = training.map(lambda l: (float(modelNB.predict(l.features)), l.label))\n",
    "metricsNB = MulticlassMetrics(predictionAndLabelsNB)\n",
    "print('Confusion Matrix for Naive Bayes : \\n\\n' + str(metricsNB.confusionMatrix().toArray()) +\n",
    "'\\n\\nSummary Of Statistics For Naive Bayes\\n\\nTotal Accuracy  : ' + str(format(metricsNB.accuracy,'.2f')) +\n",
    "'\\n\\nTotal Precision : ' + str(format(metricsNB.precision(),'.2f')) + '\\t Precision 1: ' + str(format(metricsNB.precision(0),'.2f')) + '\\t Precision 2: ' + str(format(metricsNB.precision(1),'.2f')) + '\\t Precision 3: ' + str(format(metricsNB.precision(2),'.2f')) +\n",
    "'\\n\\nTotal Recall    : ' + str(format(metricsNB.recall(),'.2f')) + '\\t Recall 1   : ' + str(format(metricsNB.recall(0),'.2f')) + '\\t Recall 2   : ' + str(format(metricsNB.recall(1),'.2f')) + '\\t Recall 3   : ' + str(format(metricsNB.recall(2),'.2f')) +\n",
    "'\\n\\nTotal fMeasure  : ' + str(format(metricsNB.fMeasure(),'.2f'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions using test data:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0), (2.0, 2.0), (0.0, 0.0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nPredictions using test data:\\n\")\n",
    "predictionAndLabels_testNB = test.map(lambda l: (float(modelNB.predict(l.features)), l.label))\n",
    "predictionAndLabels_testNB.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------+---------+-----------+--------------+-----------+------+------+------+------+------+------+-----+-----+-----+----+----+----+----+-----+----+----+----+----+-----+----+----+----+----+----+----+----+-----+----+----+----+----+-----+----+----+----+----+----+----+---+---+---+----+---+----+----+----+----+----+----+----+----+----+----+---+---+----+\n",
      "|Author|wrds_sent_mean|wrds_sent_std|Lex_diver|Commas_sent|Semicolon_sent|Colons_sent|    NN|   NNP|    DT|    IN|    JJ|   NNS|   f1|   f2|   f3|  f4|  f5|  f6|  f7|   f8|  f9| f10| f11| f12|  f13| f14| f15| f16| f17| f18| f19| f20|  bg1| bg2| bg3| bg4| bg5|  bg6| bg7| bg8| bg9|bg10| tg1| tg2|tg3|tg4|tg5| tg6|tg7| tg8| tg9|tg10| qg1| qg2| qg3| qg4| qg5| qg6| qg7|qg8|qg9|qg10|\n",
      "+------+--------------+-------------+---------+-----------+--------------+-----------+------+------+------+------+------+------+-----+-----+-----+----+----+----+----+-----+----+----+----+----+-----+----+----+----+----+----+----+----+-----+----+----+----+----+-----+----+----+----+----+----+----+---+---+---+----+---+----+----+----+----+----+----+----+----+----+----+---+---+----+\n",
      "|  null|       34.2479|      26.7626|   0.3205|     2.3223|         0.595|     0.1405|   0.5|   0.5|   0.5|   0.5|   0.5|   0.5| 20.0|  3.0|  0.0| 0.0|25.0|10.0| 5.0|  0.0| 3.0| 0.0| 4.0|11.0|  0.0| 5.0| 2.0| 2.0| 7.0|10.0| 1.0| 0.0| 66.0|26.0|13.0|18.0| 7.0|  0.0| 4.0|19.0| 5.0| 3.0| 0.0| 0.0|0.0|3.0|2.0| 0.0|0.0| 0.0| 0.0| 2.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|       23.9329|       16.993|   0.2398|     1.9597|        0.3557|     0.0805|   0.5|   0.5|   0.5|   0.5|   0.5|   0.5|  0.0| 11.0|  6.0| 0.0|11.0|25.0|24.0|  0.0| 8.0| 0.0| 2.0| 4.0| 19.0| 4.0| 1.0| 4.0| 5.0| 7.0| 3.0| 3.0| 12.0| 7.0| 3.0| 7.0|14.0|  0.0| 7.0| 7.0| 3.0|15.0| 0.0| 0.0|1.0|0.0|0.0| 0.0|6.0| 0.0| 0.0| 1.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|       32.5352|      23.0326|   0.2671|     2.8451|        0.5282|     0.0704|   0.5|   0.5|   0.5|   0.5|   0.5|   1.0|  0.0| 10.0|  2.0| 0.0|15.0|20.0|20.0|  0.0| 9.0|37.0| 0.0| 7.0|  2.0| 3.0| 5.0|11.0| 3.0|10.0| 0.0|13.0| 22.0|23.0|10.0|11.0|19.0|  0.0| 7.0| 7.0| 4.0| 9.0| 0.0| 0.0|1.0|0.0|0.0| 0.0|1.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|       24.8122|      17.1386|   0.3193|     1.6133|        0.3315|     0.0166|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|  4.0|  2.0|  8.0| 7.0|28.0| 5.0|15.0|  0.0| 8.0| 1.0| 7.0| 7.0|  0.0| 3.0| 3.0| 8.0| 2.0| 8.0| 2.0| 7.0| 42.0|23.0|10.0|10.0| 8.0|  0.0| 7.0|10.0|10.0| 3.0| 0.0| 0.0|2.0|2.0|4.0| 0.0|0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|       24.6622|      19.1433|   0.1857|     2.3198|         0.214|      0.027|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|201.0| 49.0| 85.0| 0.0|35.0|18.0|21.0|  0.0|28.0| 0.0|24.0| 8.0|100.0|18.0|18.0|13.0|10.0|11.0|29.0|20.0| 35.0|47.0|25.0|19.0|24.0| 60.0|27.0|34.0|30.0|11.0| 5.0| 5.0|3.0|5.0|1.0|20.0|1.0| 0.0| 4.0| 3.0| 0.0| 0.0| 4.0|16.0| 0.0| 0.0|11.0|9.0|0.0| 0.0|\n",
      "|  null|       25.2607|      22.9148|   0.2293|      2.344|        0.2564|     0.0171|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|327.0|246.0|119.0|15.0|21.0|21.0|10.0|  0.0|24.0|75.0|18.0|34.0|  1.0|17.0|16.0|11.0|16.0|29.0|15.0| 3.0|103.0|43.0|43.0|49.0|14.0|105.0|10.0|20.0|31.0|32.0|54.0|42.0|2.0|2.0|3.0| 0.0|0.0|19.0| 2.0| 0.0|18.0|17.0| 2.0| 0.0|14.0|12.0| 0.0|0.0|8.0| 8.0|\n",
      "|  null|        21.694|      15.0759|   0.2106|     1.3497|        0.2459|     0.0383|0.3333|   0.5|   0.5|0.3333|   0.5|   0.5|  0.0|  1.0| 32.0|34.0| 4.0|18.0|12.0|  0.0| 5.0| 1.0| 0.0| 0.0|  0.0| 8.0| 4.0|10.0| 4.0| 0.0|11.0|10.0| 12.0|21.0| 8.0|13.0| 4.0|  0.0|15.0| 8.0| 5.0| 2.0| 0.0| 0.0|4.0|1.0|0.0| 0.0|2.0| 0.0| 0.0| 3.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|        27.494|      24.0968|   0.3139|     2.4345|        0.2619|     0.0893|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|  0.0|  0.0|  7.0|93.0|18.0|13.0| 3.0|  0.0| 8.0| 2.0| 3.0|12.0|  1.0|16.0| 7.0| 7.0|15.0| 6.0| 7.0| 3.0| 25.0|26.0|17.0| 8.0|10.0|  0.0| 5.0| 5.0| 2.0| 3.0| 0.0| 0.0|1.0|0.0|1.0| 0.0|0.0| 0.0| 0.0| 3.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|       25.4818|      20.5277|   0.2959|     1.8773|        0.1727|     0.0045|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|  0.0| 32.0|  2.0| 0.0|15.0|10.0| 3.0|  0.0|18.0|16.0|15.0| 9.0|  1.0| 6.0| 8.0| 8.0|15.0| 7.0| 6.0|13.0| 47.0|36.0|18.0|13.0|17.0|  0.0| 1.0|14.0|18.0|12.0| 0.0| 0.0|0.0|3.0|2.0| 0.0|2.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|       23.3194|      19.3241|   0.2752|     1.4074|        0.2037|     0.0648|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|  1.0|  1.0| 14.0|35.0|14.0|12.0| 4.0|  0.0| 8.0| 0.0|10.0|10.0|  5.0| 6.0|15.0| 6.0|14.0| 4.0| 7.0|13.0| 22.0|25.0|12.0| 9.0| 5.0|  0.0|10.0| 3.0| 5.0| 5.0| 0.0| 0.0|1.0|5.0|0.0| 0.0|0.0| 0.0| 0.0| 2.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|       15.0116|      14.4273|   0.2022|     1.3666|        0.1995|     0.0464|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0|  0.0|  8.0| 95.0| 0.0|51.0|42.0|33.0|182.0|24.0| 6.0|52.0|23.0|  2.0|34.0|30.0|26.0|33.0|16.0|21.0|15.0| 58.0|75.0|29.0|26.0|26.0|  0.0|48.0|19.0|23.0|20.0| 0.0| 0.0|8.0|5.0|1.0| 0.0|3.0| 0.0| 3.0| 3.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|       28.9605|      20.4421|   0.2226|     2.1186|        0.4124|     0.0452|   0.5|   0.5|0.3333|   0.5|   0.5|0.3333|  4.0| 53.0| 13.0|45.0|16.0|20.0|22.0|  0.0|13.0| 4.0| 6.0|14.0|  3.0| 9.0| 9.0|12.0| 2.0|14.0|10.0|21.0| 21.0|12.0|11.0| 7.0|13.0|  0.0|22.0| 6.0| 4.0|12.0| 0.0| 0.0|5.0|0.0|3.0| 0.0|3.0| 0.0| 0.0| 1.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|        30.678|      25.6465|   0.3044|     2.1864|        0.3729|     0.0169|   0.5|0.3333|   0.5|   0.5|   0.5|   0.5|  0.0|  7.0|  4.0|29.0|14.0|10.0|20.0|  0.0| 6.0| 0.0| 3.0| 3.0|  7.0| 7.0| 7.0|10.0| 4.0| 4.0| 3.0| 3.0| 17.0|12.0| 3.0| 9.0|11.0|  0.0| 7.0| 3.0| 0.0| 0.0| 0.0| 0.0|1.0|0.0|1.0| 0.0|1.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|       25.9219|      23.1971|   0.3068|        2.0|        0.3906|     0.0156|   0.5|   0.5|   0.5|   0.5|0.3333|0.3333|  0.0| 22.0| 10.0|64.0| 7.0| 4.0| 0.0|  0.0| 5.0| 1.0| 1.0| 5.0|  2.0| 1.0| 3.0| 5.0| 1.0| 1.0|10.0| 1.0|  5.0| 3.0| 2.0| 1.0| 5.0|  0.0| 3.0| 0.0| 3.0| 3.0| 0.0| 0.0|0.0|0.0|0.0| 0.0|1.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "|  null|       19.3016|      14.6446|   0.3302|     2.2143|        0.0794|     0.0238|   0.0|   0.0|   0.0|   0.0|   0.0|   0.0| 58.0| 11.0| 19.0| 0.0| 9.0|14.0| 1.0|  0.0| 7.0|19.0|14.0| 4.0|  0.0| 4.0| 6.0| 1.0| 0.0| 2.0| 3.0| 2.0|  8.0| 8.0| 4.0| 8.0| 7.0| 18.0| 2.0| 7.0| 1.0| 3.0| 5.0|10.0|0.0|1.0|2.0| 0.0|0.0| 0.0|10.0| 0.0| 0.0| 0.0|10.0| 0.0| 0.0| 0.0| 0.0|0.0|0.0| 0.0|\n",
      "+------+--------------+-------------+---------+-----------+--------------+-----------+------+------+------+------+------+------+-----+-----+-----+----+----+----+----+-----+----+----+----+----+-----+----+----+----+----+----+----+----+-----+----+----+----+----+-----+----+----+----+----+----+----+---+---+---+----+---+----+----+----+----+----+----+----+----+----+----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df=spark.sql(\"SELECT CASE WHEN ld.Doc_Name == '1.txt' THEN 0 WHEN ld.Doc_Name == '2.txt' THEN 1 \\\n",
    "                                WHEN ld.Doc_Name == '3.txt' THEN 1 END Author, \\\n",
    "                     ld.wrds_sent_mean, ld.wrds_sent_std, ld.Lex_diver, ld.Commas_sent, ld.Semicolon_sent, ld.Colons_sent, \\\n",
    "                     sd.NN, sd.NNP, sd.DT, sd.IN, sd.JJ, sd.NNS, wd.f1, wd.f2,  wd.f3, wd.f4, wd.f5, wd.f6, wd.f7, wd.f8, \\\n",
    "                     wd.f9, wd.f10, wd.f11, wd.f12, wd.f13, wd.f14, wd.f15, wd.f16, wd.f17, wd.f18, wd.f19, wd.f20, bd.bg1,\\\n",
    "                     bd.bg2, bd.bg3, bd.bg4, bd.bg5, bd.bg6, bd.bg7, bd.bg8, bd.bg9, bd.bg10, td.tg1, td.tg2, td.tg3, td.tg4,\\\n",
    "                     td.tg5, td.tg6, td.tg7, td.tg8, td.tg9, td.tg10, qd.qg1, qd.qg2, qd.qg3, qd.qg4, qd.qg5, qd.qg6, qd.qg7, \\\n",
    "                     qd.qg8, qd.qg9, qd.qg10 FROM lexical_df ld, syntactic_df sd, word_df wd, bigram_df bd, \\\n",
    "                     trigram_df td, qgram_df qd WHERE ld.Doc_Name = sd.Doc_Name and ld.Doc_Name = wd.Doc_Name \\\n",
    "                     and ld.Doc_Name = bd.Doc_Name and ld.Doc_Name = td.Doc_Name and ld.Doc_Name = qd.Doc_Name\")\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.repartition(1).write.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').save(\"C:/Bigdata/BookAuthor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Author=1, wrds_sent_mean=23.0, wrds_sent_std=9.3808, Lex_diver=0.6783, Commas_sent=1.6, Semicolon_sent=0.0, Colons_sent=0.0, NN=0.0, NNP=0.0, DT=0.0, IN=0.0, JJ=0.0, NNS=0.0, f1=1.0, f2=0.0, f3=0.0, f4=2.0, f5=2.0, f6=1.0, f7=0.0, f8=2.0, f9=0.0, f10=0.0, f11=0.0, f12=0.0, f13=0.0, f14=1.0, f15=2.0, f16=2.0, f17=0.0, f18=0.0, f19=0.0, f20=0.0, bg1=2.0, bg2=2.0, bg3=0.0, bg4=0.0, bg5=0.0, bg6=1.0, bg7=0.0, bg8=0.0, bg9=0.0, bg10=1.0, tg1=0.0, tg2=1.0, tg3=1.0, tg4=0.0, tg5=0.0, tg6=0.0, tg7=0.0, tg8=0.0, tg9=0.0, tg10=0.0, qg1=0.0, qg2=0.0, qg3=0.0, qg4=0.0, qg5=1.0, qg6=0.0, qg7=0.0, qg8=0.0, qg9=0.0, qg10=0.0),\n",
       " Row(Author=0, wrds_sent_mean=16.0, wrds_sent_std=10.8934, Lex_diver=0.5694, Commas_sent=0.4444, Semicolon_sent=0.1111, Colons_sent=0.0, NN=0.5, NNP=0.5, DT=0.3333, IN=0.5, JJ=0.3333, NNS=0.5, f1=0.0, f2=1.0, f3=3.0, f4=0.0, f5=0.0, f6=0.0, f7=1.0, f8=0.0, f9=2.0, f10=0.0, f11=2.0, f12=0.0, f13=1.0, f14=0.0, f15=0.0, f16=0.0, f17=0.0, f18=2.0, f19=2.0, f20=0.0, bg1=0.0, bg2=1.0, bg3=1.0, bg4=2.0, bg5=2.0, bg6=0.0, bg7=2.0, bg8=0.0, bg9=2.0, bg10=0.0, tg1=0.0, tg2=0.0, tg3=0.0, tg4=0.0, tg5=0.0, tg6=0.0, tg7=0.0, tg8=0.0, tg9=0.0, tg10=1.0, qg1=1.0, qg2=1.0, qg3=1.0, qg4=0.0, qg5=0.0, qg6=1.0, qg7=0.0, qg8=1.0, qg9=1.0, qg10=1.0),\n",
       " Row(Author=1, wrds_sent_mean=17.8333, wrds_sent_std=7.3918, Lex_diver=0.5654, Commas_sent=1.0833, Semicolon_sent=0.25, Colons_sent=0.0, NN=0.5, NNP=0.3333, DT=0.5, IN=0.5, JJ=0.5, NNS=0.5, f1=4.0, f2=2.0, f3=0.0, f4=1.0, f5=1.0, f6=2.0, f7=2.0, f8=0.0, f9=0.0, f10=2.0, f11=0.0, f12=2.0, f13=1.0, f14=1.0, f15=0.0, f16=0.0, f17=2.0, f18=0.0, f19=0.0, f20=2.0, bg1=6.0, bg2=0.0, bg3=2.0, bg4=0.0, bg5=0.0, bg6=1.0, bg7=0.0, bg8=2.0, bg9=0.0, bg10=1.0, tg1=2.0, tg2=1.0, tg3=0.0, tg4=1.0, tg5=1.0, tg6=1.0, tg7=1.0, tg8=1.0, tg9=1.0, tg10=0.0, qg1=0.0, qg2=0.0, qg3=0.0, qg4=1.0, qg5=0.0, qg6=0.0, qg7=1.0, qg8=0.0, qg9=0.0, qg10=0.0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('C:/Bigdata/Book_Author.csv')\n",
    "#df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [23.0,9.3808,0.6783,1.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,2.0,2.0,1.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,1.0,2.0,2.0,0.0,0.0,0.0,0.0,2.0,2.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [16.0,10.8934,0.5694,0.4444,0.1111,0.0,0.5,0.5,0.3333,0.5,0.3333,0.5,0.0,1.0,3.0,0.0,0.0,0.0,1.0,0.0,2.0,0.0,2.0,0.0,1.0,0.0,0.0,0.0,0.0,2.0,2.0,0.0,0.0,1.0,1.0,2.0,2.0,0.0,2.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,1.0,1.0,1.0]),\n",
       " LabeledPoint(1.0, [17.8333,7.3918,0.5654,1.0833,0.25,0.0,0.5,0.3333,0.5,0.5,0.5,0.5,4.0,2.0,0.0,1.0,1.0,2.0,2.0,0.0,0.0,2.0,0.0,2.0,1.0,1.0,0.0,0.0,2.0,0.0,0.0,2.0,6.0,0.0,2.0,0.0,0.0,1.0,0.0,2.0,0.0,1.0,2.0,1.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp= df.rdd.map(lambda row: LabeledPoint(row[0], row[1:]))\n",
    "lp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(lp)\n",
    "\n",
    "predictionAndLabels = lp.map(lambda l: (float(model.predict(l.features)), l.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "\n",
      "[[ 1.  0.]\n",
      " [ 0.  2.]]\n",
      "\n",
      "\n",
      "Total Accuracy  : 1.0\n"
     ]
    }
   ],
   "source": [
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "print('Confusion Matrix : \\n\\n' + str(metrics.confusionMatrix().toArray()))\n",
    "print('\\n\\nTotal Accuracy  : ' + str(metrics.accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|                text|               words|             bigrams|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|1.txt|i the knighted kn...|[i, the, knighted...|[i the, the knigh...|\n",
      "|2.txt|after a brief res...|[after, a, brief,...|[after a, a brief...|\n",
      "|3.txt|frankfort is one ...|[frankfort, is, o...|[frankfort is, is...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "bigramDF = bigram.transform(wordsDataFrame)\n",
    "bigramDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of the', 8),\n",
       " ('in the', 3),\n",
       " ('the german', 3),\n",
       " ('would have', 2),\n",
       " ('i was', 2),\n",
       " ('had been', 2),\n",
       " ('birthplace of', 2),\n",
       " ('to be', 2),\n",
       " ('was in', 2),\n",
       " ('to learn', 2),\n",
       " ('me in', 2),\n",
       " ('at frankfort', 2),\n",
       " ('at the', 2),\n",
       " ('we made', 2),\n",
       " ('the first', 2),\n",
       " ('the birthplace', 2),\n",
       " ('it this', 2),\n",
       " ('he was', 2),\n",
       " ('for a', 2),\n",
       " ('it occurred', 2),\n",
       " ('mr harris', 2),\n",
       " ('her judging', 1),\n",
       " ('chased by', 1),\n",
       " ('to visit', 1),\n",
       " ('not be', 1),\n",
       " ('as i', 1),\n",
       " ('followed so', 1),\n",
       " ('have given', 1),\n",
       " ('buchstaben they', 1),\n",
       " ('interesting city', 1),\n",
       " ('to commemorate', 1),\n",
       " ('he watched', 1),\n",
       " ('an hour', 1),\n",
       " ('man adventurous', 1),\n",
       " ('afforded the', 1),\n",
       " ('gracing and', 1),\n",
       " ('many years', 1),\n",
       " ('dawn in', 1),\n",
       " ('charlemagne while', 1),\n",
       " ('distinction of', 1),\n",
       " ('instead of', 1),\n",
       " ('be done', 1),\n",
       " ('very badly', 1),\n",
       " ('occurred at', 1),\n",
       " ('german word', 1),\n",
       " ('that she', 1),\n",
       " ('moment we', 1),\n",
       " ('were either', 1),\n",
       " ('honor of', 1),\n",
       " ('knighted knave', 1),\n",
       " ('preparations for', 1),\n",
       " ('the site', 1),\n",
       " ('said arrived', 1),\n",
       " ('evidence that', 1),\n",
       " ('been afforded', 1),\n",
       " ('first place', 1),\n",
       " ('frankfort has', 1),\n",
       " ('other cities', 1),\n",
       " ('the army', 1),\n",
       " ('spectacle of', 1),\n",
       " ('private parties', 1),\n",
       " ('southward in', 1),\n",
       " ('types were', 1),\n",
       " ('was gained', 1),\n",
       " ('i desired', 1),\n",
       " ('pedestrian trip', 1),\n",
       " ('been kept', 1),\n",
       " ('said or', 1),\n",
       " ('been many', 1),\n",
       " ('has another', 1),\n",
       " ('guide but', 1),\n",
       " ('incident occurred', 1),\n",
       " ('young approach', 1),\n",
       " ('me that', 1),\n",
       " ('over and', 1),\n",
       " ('art as', 1),\n",
       " ('army followed', 1),\n",
       " ('word for', 1),\n",
       " ('or being', 1),\n",
       " ('occurred charlemagne', 1),\n",
       " ('paint i', 1),\n",
       " ('chasing the', 1),\n",
       " ('city i', 1),\n",
       " ('that i', 1),\n",
       " ('her young', 1),\n",
       " ('we changed', 1),\n",
       " ('of agent', 1),\n",
       " ('rest at', 1),\n",
       " ('river at', 1),\n",
       " ('approach the', 1),\n",
       " ('to accompany', 1),\n",
       " ('for it', 1),\n",
       " ('city permits', 1),\n",
       " ('made on', 1),\n",
       " ('the bank', 1),\n",
       " ('followed by', 1),\n",
       " ('program for', 1),\n",
       " ('hour in', 1),\n",
       " ('waded over', 1),\n",
       " ('this is', 1),\n",
       " ('water he', 1),\n",
       " ('undertake a', 1),\n",
       " ('distinction it', 1),\n",
       " ('one day', 1),\n",
       " ('a fog', 1),\n",
       " ('cities where', 1),\n",
       " ('permits this', 1),\n",
       " ('i the', 1),\n",
       " ('watched her', 1),\n",
       " ('brief rest', 1),\n",
       " ('after much', 1),\n",
       " ('ford and', 1),\n",
       " ('but it', 1),\n",
       " ('franks none', 1),\n",
       " ('as they', 1),\n",
       " ('a journey', 1),\n",
       " ('judging that', 1),\n",
       " ('with the', 1),\n",
       " ('get across', 1),\n",
       " ('in order', 1),\n",
       " ('parties instead', 1),\n",
       " ('first movable', 1),\n",
       " ('ford of', 1),\n",
       " ('defeat was', 1),\n",
       " ('the express', 1),\n",
       " ('furnish to', 1),\n",
       " ('which have', 1),\n",
       " ('of possessing', 1),\n",
       " ('would seek', 1),\n",
       " ('he named', 1),\n",
       " ('birch sticks', 1),\n",
       " ('for this', 1),\n",
       " ('i looked', 1),\n",
       " ('great frankish', 1),\n",
       " ('decided that', 1),\n",
       " ('and took', 1),\n",
       " ('person to', 1),\n",
       " ('another distinction', 1),\n",
       " ('the world', 1),\n",
       " ('victory or', 1),\n",
       " ('which he', 1),\n",
       " ('it is', 1),\n",
       " ('instead the', 1),\n",
       " ('commanded a', 1),\n",
       " ('following incident', 1),\n",
       " ('art while', 1),\n",
       " ('have the', 1),\n",
       " ('bank of', 1),\n",
       " ('me for', 1),\n",
       " ('she waded', 1),\n",
       " ('the house', 1),\n",
       " ('to mankind', 1),\n",
       " ('him or', 1),\n",
       " ('private reasons', 1),\n",
       " ('sticks buchstabe', 1),\n",
       " ('determined to', 1),\n",
       " ('sympathy with', 1),\n",
       " ('was also', 1),\n",
       " ('i determined', 1),\n",
       " ('we spent', 1),\n",
       " ('thought i', 1),\n",
       " ('was right', 1),\n",
       " ('wanted to', 1),\n",
       " ('years since', 1),\n",
       " ('learn to', 1),\n",
       " ('one of', 1),\n",
       " ('none was', 1),\n",
       " ('in a', 1),\n",
       " ('that the', 1),\n",
       " ('the right', 1),\n",
       " ('the knighted', 1),\n",
       " ('they say', 1),\n",
       " ('found it', 1),\n",
       " ('the river', 1),\n",
       " ('dignifying herself', 1),\n",
       " ('bergen one', 1),\n",
       " ('since the', 1),\n",
       " ('and finally', 1),\n",
       " ('the ford', 1),\n",
       " ('changed the', 1),\n",
       " ('took the', 1),\n",
       " ('of gutenburg', 1),\n",
       " ('he wanted', 1),\n",
       " ('the soft', 1),\n",
       " ('being chased', 1),\n",
       " ('and found', 1),\n",
       " ('or behind', 1),\n",
       " ('a brief', 1),\n",
       " ('agent and', 1),\n",
       " ('the main', 1),\n",
       " ('sort of', 1),\n",
       " ('this spectacle', 1),\n",
       " ('at dawn', 1),\n",
       " ('named for', 1),\n",
       " ('day it', 1),\n",
       " ('the last', 1),\n",
       " ('purpose to', 1),\n",
       " ('this he', 1),\n",
       " ('a short', 1),\n",
       " ('sixteen cities', 1),\n",
       " ('made a', 1),\n",
       " ('by them', 1),\n",
       " ('no memorandum', 1),\n",
       " ('none of', 1),\n",
       " ('hamburg we', 1),\n",
       " ('the name', 1),\n",
       " ('a person', 1),\n",
       " ('and dignifying', 1),\n",
       " ('commemorate the', 1),\n",
       " ('happened were', 1),\n",
       " ('spectacle so', 1),\n",
       " ('an enthusiast', 1),\n",
       " ('and not', 1),\n",
       " ('march i', 1),\n",
       " ('good evidence', 1),\n",
       " ('short halt', 1),\n",
       " ('given anything', 1),\n",
       " ('has been', 1),\n",
       " ('that frankfort', 1),\n",
       " ('in art', 1),\n",
       " ('service it', 1),\n",
       " ('on birch', 1),\n",
       " ('did harris', 1),\n",
       " ('being the', 1),\n",
       " ('do it', 1),\n",
       " ('the sixteen', 1),\n",
       " ('an interesting', 1),\n",
       " ('so i', 1),\n",
       " ('europe on', 1),\n",
       " ('fog the', 1),\n",
       " ('alphabet or', 1),\n",
       " ('either before', 1),\n",
       " ('is the', 1),\n",
       " ('made preparations', 1),\n",
       " ('long pedestrian', 1),\n",
       " ('so did', 1),\n",
       " ('desired to', 1),\n",
       " ('anxious to', 1),\n",
       " ('a man', 1),\n",
       " ('house to', 1),\n",
       " ('belong to', 1),\n",
       " ('while chasing', 1),\n",
       " ('it an', 1),\n",
       " ('the water', 1),\n",
       " ('on the', 1),\n",
       " ('and in', 1),\n",
       " ('finally hired', 1),\n",
       " ('least of', 1),\n",
       " ('frankfort on', 1),\n",
       " ('harris for', 1),\n",
       " ('he said', 1),\n",
       " ('across very', 1),\n",
       " ('there which', 1),\n",
       " ('this house', 1),\n",
       " ('have liked', 1),\n",
       " ('the goethe', 1),\n",
       " ('for private', 1),\n",
       " ('to belong', 1),\n",
       " ('of person', 1),\n",
       " ('in march', 1),\n",
       " ('saw a', 1),\n",
       " ('foot after', 1),\n",
       " ('in europe', 1),\n",
       " ('a long', 1),\n",
       " ('before him', 1),\n",
       " ('of a', 1),\n",
       " ('and he', 1),\n",
       " ('gained or', 1),\n",
       " ('were made', 1),\n",
       " ('charlemagne commanded', 1),\n",
       " ('but in', 1),\n",
       " ('soft spring', 1),\n",
       " ('knave of', 1),\n",
       " ('world had', 1),\n",
       " ('on foot', 1),\n",
       " ('harris was', 1),\n",
       " ('for the', 1),\n",
       " ('gutenburg but', 1),\n",
       " ('while in', 1),\n",
       " ('the program', 1),\n",
       " ('be built', 1),\n",
       " ('kept so', 1),\n",
       " ('the saxons', 1),\n",
       " ('that it', 1),\n",
       " ('journey through', 1),\n",
       " ('so a', 1),\n",
       " ('place it', 1),\n",
       " ('arrived at', 1),\n",
       " ('of bergen', 1),\n",
       " ('order to', 1),\n",
       " ('right sort', 1),\n",
       " ('the episode', 1),\n",
       " ('reasons and', 1),\n",
       " ('or defeat', 1),\n",
       " ('looked about', 1),\n",
       " ('person fitted', 1),\n",
       " ('was as', 1),\n",
       " ('in sympathy', 1),\n",
       " ('i decided', 1),\n",
       " ('about me', 1),\n",
       " ('deer followed', 1),\n",
       " ('episode charlemagne', 1),\n",
       " ('the franks', 1),\n",
       " ('a mr', 1),\n",
       " ('the other', 1),\n",
       " ('much thought', 1),\n",
       " ('they said', 1),\n",
       " ('weather but', 1),\n",
       " ('at hamburg', 1),\n",
       " ('the capacity', 1),\n",
       " ('badly he', 1),\n",
       " ('enemy were', 1),\n",
       " ('is good', 1),\n",
       " ('german alphabet', 1),\n",
       " ('mansion instead', 1),\n",
       " ('by her', 1),\n",
       " ('goethe mansion', 1),\n",
       " ('train we', 1),\n",
       " ('of being', 1),\n",
       " ('seek a', 1),\n",
       " ('protecting it', 1),\n",
       " ('where this', 1),\n",
       " ('could not', 1),\n",
       " ('city to', 1),\n",
       " ('where the', 1),\n",
       " ('a guide', 1),\n",
       " ('named frankfort', 1),\n",
       " ('to get', 1),\n",
       " ('fitted to', 1),\n",
       " ('or avoided', 1),\n",
       " ('visit the', 1),\n",
       " ('avoided and', 1),\n",
       " ('for alphabet', 1),\n",
       " ('after a', 1),\n",
       " ('halt at', 1),\n",
       " ('site of', 1),\n",
       " ('movable types', 1),\n",
       " ('mankind this', 1),\n",
       " ('it had', 1),\n",
       " ('to paint', 1),\n",
       " ('place where', 1),\n",
       " ('spring weather', 1),\n",
       " ('he saw', 1),\n",
       " ('spent an', 1),\n",
       " ('enthusiast in', 1),\n",
       " ('also my', 1),\n",
       " ('cities which', 1),\n",
       " ('the city', 1),\n",
       " ('alphabet buchstaben', 1),\n",
       " ('a city', 1),\n",
       " ('a deer', 1),\n",
       " ('herself with', 1),\n",
       " ('europe mr', 1),\n",
       " ('to furnish', 1),\n",
       " ('it was', 1),\n",
       " ('the spectacle', 1),\n",
       " ('i would', 1),\n",
       " ('this event', 1),\n",
       " ('or at', 1),\n",
       " ('not less', 1),\n",
       " ('a ford', 1),\n",
       " ('done as', 1),\n",
       " ('was and', 1),\n",
       " ('less anxious', 1),\n",
       " ('it could', 1),\n",
       " ('as much', 1),\n",
       " ('buchstabe hence', 1),\n",
       " ('say that', 1),\n",
       " ('a great', 1),\n",
       " ('to me', 1),\n",
       " ('the honor', 1),\n",
       " ('had presently', 1),\n",
       " ('to undertake', 1),\n",
       " ('at least', 1),\n",
       " ('hence the', 1),\n",
       " ('built there', 1),\n",
       " ('was a', 1),\n",
       " ('and protecting', 1),\n",
       " ('my purpose', 1),\n",
       " ('frankish victory', 1),\n",
       " ('was to', 1),\n",
       " ('hired a', 1),\n",
       " ('this service', 1),\n",
       " ('this was', 1),\n",
       " ('was the', 1),\n",
       " ('to do', 1),\n",
       " ('so we', 1),\n",
       " ('trip southward', 1),\n",
       " ('anything for', 1),\n",
       " ('of an', 1),\n",
       " ('of gracing', 1),\n",
       " ('them as', 1),\n",
       " ('to study', 1),\n",
       " ('event happened', 1),\n",
       " ('study art', 1),\n",
       " ('last moment', 1),\n",
       " ('behind him', 1),\n",
       " ('him but', 1),\n",
       " ('much of', 1),\n",
       " ('german language', 1),\n",
       " ('were named', 1),\n",
       " ('right she', 1),\n",
       " ('presently he', 1),\n",
       " ('and the', 1),\n",
       " ('adventurous enough', 1),\n",
       " ('learn the', 1),\n",
       " ('is one', 1),\n",
       " ('capacity of', 1),\n",
       " ('through europe', 1),\n",
       " ('but at', 1),\n",
       " ('house has', 1),\n",
       " ('in any', 1),\n",
       " ('enough to', 1),\n",
       " ('occurred to', 1),\n",
       " ('main and', 1),\n",
       " ('liked to', 1),\n",
       " ('he would', 1),\n",
       " ('as he', 1),\n",
       " ('be had', 1),\n",
       " ('possessing and', 1),\n",
       " ('with me', 1),\n",
       " ('language so', 1),\n",
       " ('frankfort the', 1),\n",
       " ('case he', 1),\n",
       " ('the distinction', 1),\n",
       " ('as no', 1),\n",
       " ('memorandum of', 1),\n",
       " ('she would', 1),\n",
       " ('express train', 1),\n",
       " ('the place', 1),\n",
       " ('any case', 1),\n",
       " ('to private', 1),\n",
       " ('but none', 1),\n",
       " ('frankfort is', 1),\n",
       " ('saxons as', 1),\n",
       " ('in this', 1),\n",
       " ('the following', 1),\n",
       " ('frankfort was', 1),\n",
       " ('accompany me', 1),\n",
       " ('the enemy', 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipair = bigramDF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "bipair.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of the', 'in the', 'the german', 'would have', 'i was', 'had been', 'birthplace of', 'to be', 'was in']\n"
     ]
    }
   ],
   "source": [
    "bistp = bipair.map(lambda x: x[0]).collect()[0:9]\n",
    "print(bistp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1.txt',\n",
       "  ['in the',\n",
       "   'the german',\n",
       "   'i was',\n",
       "   'i was',\n",
       "   'had been',\n",
       "   'had been',\n",
       "   'was in',\n",
       "   'was in']),\n",
       " ('2.txt',\n",
       "  ['of the', 'of the', 'in the', 'in the', 'would have', 'birthplace of']),\n",
       " ('3.txt',\n",
       "  ['of the',\n",
       "   'of the',\n",
       "   'of the',\n",
       "   'of the',\n",
       "   'of the',\n",
       "   'of the',\n",
       "   'the german',\n",
       "   'the german',\n",
       "   'would have',\n",
       "   'birthplace of',\n",
       "   'to be',\n",
       "   'to be'])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filrdd = bigramDF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_test(l,bistp))\n",
    "filrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_test(text, lst):\n",
    "\tfile = text[0]\n",
    "\tbg = text[1]\n",
    "\tres=[]\n",
    "\tfor i in lst:\n",
    "\t\tfor j in bg:\n",
    "\t\t\tif i==j:\n",
    "\t\t\t\tres.append(i)\n",
    "\treturn (file,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biremover = StopWordsRemover(inputCol=\"bigrams\", outputCol=\"finalword\", stopWords=bistp)\n",
    "biFDF = biremover.transform(bigramDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|Doc_Name| f1|f10|f11|f12|f13|f14|f15|f16|f17|f18|f19| f2|f20| f3| f4| f5| f6| f7| f8| f9|\n",
      "+--------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|   1.txt|0.0|2.0|0.0|2.0|0.0|0.0|0.0|2.0|0.0|2.0|0.0|1.0|0.0|0.0|0.0|0.0|1.0|3.0|1.0|0.0|\n",
      "|   2.txt|1.0|0.0|2.0|0.0|2.0|0.0|2.0|0.0|0.0|0.0|0.0|0.0|0.0|2.0|2.0|1.0|0.0|0.0|0.0|1.0|\n",
      "|   3.txt|4.0|0.0|0.0|0.0|0.0|2.0|0.0|0.0|2.0|0.0|2.0|2.0|2.0|1.0|1.0|2.0|2.0|0.0|1.0|1.0|\n",
      "+--------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "bicv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "bimodel = cv.fit(biFDF)\n",
    "biresult = bimodel.transform(biFDF)\n",
    "bifinaldata = biresult.select(\"label\",\"features\")\n",
    "\n",
    "bigram_df = bifinaldata.rdd.map(lambda x: Row(Doc_Name=x[0],bg1=float(x[1].toArray()[0]),bg2=float(x[1].toArray()[1]),bg3=float(x[1].toArray()[2]),bg4=float(x[1].toArray()[3]),bg5=float(x[1].toArray()[4]),bg6=float(x[1].toArray()[5]),bg7=float(x[1].toArray()[6]),bg8=float(x[1].toArray()[7]),bg9=float(x[1].toArray()[8]),bg10=float(x[1].toArray()[9]))).toDF()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
